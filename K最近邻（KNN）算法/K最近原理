K最近邻(K-NearestNeighbor)--K个最近的邻居
每个样本都可以用它最近的K个邻居来代表
核心思想：如果一个样本在特征空间中的K个最相邻的样本中的大多数属于某一个类别，则该样本也属于这个类别，并具有这个类别样本上的特性。
KNN算法的结果很大程度取决于K的选择

KNN算法三要素
    1、K值的选择
    2、距离度量的方式
    3、分类决策规则



1、K值的选择
    K值的选择没有固定的经验，【经验规则：K一般低于训练样本数的平方根】
    一般根据样本的分布，选择一个较小的值，
    可以通过交叉验证（cross validation）选择一个合适的K值。【交叉验证： 重复的使用数据，把得到的样布数据进行切分，组合成不同的训练集和测试集，
                                                            用训练集训练模式，用测试集来评估预测结果的好坏，在此基础上可以得到多组不同的训练集和测试集，某样本既能作为训练集，也能用作测试集，
                                                            分为三种：1、简单交叉验证；2、S折交叉验证(S-Folder cross validation)；3、留一交叉验证（样本数n<50）,还有一种自助法（很少用，样本少于20）】
    如果选择较小的K值，就相当于用较小的领域中训练出来的实例去预测，训练误差会减小,容易发生过拟合。
    如果选择较大的K值，就相当于用较大的领域训练出来的实例进行预测，其优点是可以减小泛化误差，缺点是训练误差会增大。
     贝叶斯准则（BIC）：


2、距离度量的方式
    距离一般使用欧式距离或曼哈顿距离。

   ① 欧式距离（又称欧几里得距离或欧几里得度量）：以空间为基准的两点之间最短距离，初中知识两点之间直线最短，就是求空间的两个点之间的直线距离。
    n维空间点a(x11,x12,…,x1n)与b(x21,x22,…,x2n)间的欧氏距离（两个n维向量）:d(X1,X2)=〖√(〖(X〗_1K-X_2K ))〗^2 ,勾股定理
    ②曼哈顿距离 （也称城市街区距离）：出租车几何或曼哈顿距离（manhattan distance）是十九世纪的赫尔曼·闵可夫斯基所创词汇，是种使用在几何度量的几何用语，用以标明两点在标准坐标系上的绝对轴距总和。
    ③切比雪夫

    为什么KNN选择使用欧式距离？
       因为曼哈顿距离只是平面距离，水平和垂直距离两个维度，KNN很多时候考虑的是多维度的距离，而欧氏距离可以计算任意空间里两点之间的真实距离，不受维度的限制，KNN大部分的数据不只是二维的，
       数据点都分布在不同的维度，所以欧式距离是最好的选择。

3、分类决策规则
    服从“少数服从多数”，本质上讲就是哪类样本与待分类样本最相似，就分给哪一类。

4、回归
    通过找出一个样本的K个最近邻居，将这些邻居的某个（些）属性的平均值赋给该样本，就可以得到该样本对应的属性值。

    分类模型和回归模型本质一样，分类模型是将回归模型的输出离散化。
    回归问题通常是用来预测一个值，回归是对真实值的一种逼近预测。
    分类问题是用于将事物打上一个标签，通常为离散值，分类并没有逼近的概念，最终正确结果只有一个，错误就是错误的，不会有逼近的的概念。

    定量输出称为回归，或者说连续变量预测，预测明天的气温是多少度，这是一个回归任务
    定性输出称为分类







